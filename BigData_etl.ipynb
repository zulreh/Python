{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0af31ed3-2377-4a0c-b88a-f81424c38edc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfe5888-c9f2-4b89-b3ec-eed8ddb8f6dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    #verifica se há conexão com com o sf\n",
    "    def __init__(self, host, user, password,warehouse):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.warehouse = warehouse\n",
    "        database = \"SNOWFLAKE_SAMPLE_DATA\"\n",
    "        schema = \"TPCH_SF1\"\n",
    "        dbtable = \"CUSTOMER\"\n",
    "\n",
    "        try:\n",
    "            self.session = (spark.read\\\n",
    "            .format(\"snowflake\")\\\n",
    "            .option(\"host\", host)\\\n",
    "            .option(\"user\", user)\\\n",
    "            .option(\"password\", password)\\\n",
    "            .option(\"sfWarehouse\", warehouse)\\\n",
    "            .option(\"database\", database)\\\n",
    "            .option(\"schema\", schema)\\\n",
    "            .option(\"dbtable\", dbtable)\\\n",
    "            .load()\n",
    "            )\n",
    "            self.is_connected = True\n",
    "        except Exception as e:\n",
    "            self.is_connected = False\n",
    "            print(e)\n",
    "\n",
    "        if self.is_connected:\n",
    "            print(\"Conexão ao Snowflake estabelecida com sucesso!\")\n",
    "        else:\n",
    "            print(\"Não foi possível conectar ao Snowflake.\")\n",
    "\n",
    "\n",
    "    #faz a leitura de um df no sf\n",
    "    def read_df(self,database,schema,table,query = None):\n",
    "        try:\n",
    "            if query==None:\n",
    "                df=spark.read \\\n",
    "                .format(\"snowflake\") \\\n",
    "                .option(\"host\", self.host) \\\n",
    "                .option(\"user\", self.user) \\\n",
    "                .option(\"password\", self.password) \\\n",
    "                .option(\"sfWarehouse\", self.warehouse) \\\n",
    "                .option(\"database\", database)\\\n",
    "                .option(\"schema\", schema) \\\n",
    "                .option(\"dbtable\", table)\\\n",
    "                .load()\n",
    "            elif (query!=None):\n",
    "                df=spark.read \\\n",
    "                .format(\"snowflake\") \\\n",
    "                .option(\"host\", self.host) \\\n",
    "                .option(\"user\", self.user) \\\n",
    "                .option(\"password\", self.password) \\\n",
    "                .option(\"sfWarehouse\", self.warehouse) \\\n",
    "                .option(\"database\", database) \\\n",
    "                .option(\"schema\", schema) \\\n",
    "                .option(\"query\", query) \\\n",
    "                .load()\n",
    "            return df\n",
    "        except:\n",
    "            print(\"error\")\n",
    "\n",
    "    #escreve o df no sf\n",
    "    #necessita de criar uma database e schema no sf ou usar uma já existente\n",
    "    def write_df(self,database,schema,dbtable,df):\n",
    "        \n",
    "        time_before = datetime.now()\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .option(\"host\", self.host) \\\n",
    "            .option(\"user\", self.user) \\\n",
    "            .option(\"password\", self.password) \\\n",
    "            .option(\"sfWarehouse\", self.warehouse) \\\n",
    "            .option(\"database\",database) \\\n",
    "            .option(\"schema\", schema) \\\n",
    "            .option(\"dbtable\", dbtable) \\\n",
    "            .save()\n",
    "        \n",
    "        \n",
    "        time_after = datetime.now()\n",
    "        dif_time = time_after - time_before\n",
    "        \n",
    "        result_dict = {\n",
    "            \"Tempo total transcorrido:\": dif_time.seconds,\n",
    "            \"Schema:\": schema,\n",
    "            \"Tabela:\": dbtable,\n",
    "            \"Numero de colunas:\": len(df.columns),\n",
    "            \"Nome das colunas:\": df.columns,\n",
    "            \"Numero de linhas:\": df.count() \n",
    "        }\n",
    "\n",
    "        return result_dict"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hernani_luz_etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
